# Ollama Local Model Configuration
# No API key needed - completely free local inference
LOCAL_MODEL_NAME=tinyllama
OLLAMA_BASE_URL=http://localhost:11434

# Configuration settings
DEBUG=True
LOG_LEVEL=INFO

# Enable GraphRAG visualization
ENABLE_VISUALIZATION=True
